{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LaHvKI9hkFw",
        "outputId": "69270619-7810-4e56-b3aa-f2ad1aded57c"
      },
      "source": [
        "s=input(\"Enter a String  \")\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a String  Brain tumor segmentation overall survival prediction glioma grading\n",
            "Brain tumor segmentation overall survival prediction glioma grading\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQajFHA1lllA",
        "outputId": "8855ec35-c9a4-4d82-925d-594994480bbd"
      },
      "source": [
        "import nltk, string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('punkt') # if necessary...\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
        "import json\n",
        "def read_file(filename): \n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "      \n",
        "    except IOError:\n",
        "        print(\"Error opening or reading input file: \", filename)\n",
        "        sys.exit()\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(item) for item in tokens]\n",
        "\n",
        "'''remove punctuation, lowercase, stem'''\n",
        "def normalize(text):\n",
        "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
        "\n",
        "def cosine_sim(text1, text2):\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    return ((tfidf * tfidf.T).A)[0,1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJv-ADEtl2Ei",
        "outputId": "09909bc3-da9d-41c4-899b-960bc0de3cc5"
      },
      "source": [
        "paper=[]\n",
        "paper.append(read_file('/content/paper1.json'))\n",
        "paper.append(read_file('/content/paper2.json'))\n",
        "# paper.append(read_file('/content/paper3.json'))\n",
        "for p in paper:\n",
        "  # print(p)\n",
        "  paper_similarity=0\n",
        "  for sec in p:\n",
        "    # print(p[sec])\n",
        "    paper_similarity=paper_similarity+(cosine_sim(s, p[sec]))\n",
        "  print(paper_similarity/100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.010295772310988195\n",
            "0.00957109894360639\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}